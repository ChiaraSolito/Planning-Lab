{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c707f35e",
   "metadata": {},
   "source": [
    "# Exam Planning-Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81dac69",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233976dd",
   "metadata": {},
   "source": [
    "Consider the following environment:\n",
    "\n",
    "<img src=\"images/env1.png\" alt=\"ex1\" style=\"zoom: 40%;\" />\n",
    "\n",
    "The agent starts in cell $(0, 0)$ and has to reach the treasure in $(2, 3)$. In addition to the walls of the previous environments, the floor is covered with lava, however the agent can resist to high temperature and it can use the heat to recharge its batteries, hence it receives a positive reward for being on a lava cell. The environment is deterministic and the agent must avoid the black pits of death (cells $(0,3)$, $(1, 3)$, $(1,1)$). \n",
    "Assume that you do not have access to the motion model and to reward and that the problem is undiscounted (i.e., $\\gamma$=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5be1e677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['S' 'L' 'L' 'P']\n",
      " ['L' 'P' 'L' 'P']\n",
      " ['L' 'L' 'L' 'G']]\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "module_path = os.path.abspath(os.path.join('tools'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import tools.gym, tools.envs\n",
    "from tools.utils.ai_lab_functions import *\n",
    "from timeit import default_timer as timer\n",
    "from tqdm import tqdm as tqdm\n",
    "import numpy as np\n",
    "\n",
    "def epsilon_greedy(q, state, epsilon):\n",
    "    \"\"\"\n",
    "    Epsilon-greedy action selection function\n",
    "    \n",
    "    Args:\n",
    "        q: q table\n",
    "        state: agent's current state\n",
    "        epsilon: epsilon parameter\n",
    "    \n",
    "    Returns:\n",
    "        action id\n",
    "    \"\"\"\n",
    "    an = q.shape[1]\n",
    "    probs = np.full(an, epsilon / an)\n",
    "    probs[q[state].argmax()] += 1 - epsilon\n",
    "    return np.random.choice(an, p=probs)\n",
    "\n",
    "\n",
    "env = tools.gym.make('LavaFloor-v0')\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc195b2",
   "metadata": {},
   "source": [
    "### 1.1) Given the environment reported above, find a policy with a suitable algorithm of your choice. Print the resulting policy (you can use the provided code for this)\n",
    "In this particular case since the agent doesnt need to many requirements\n",
    "Q-learning requires less to guarantee convergence, but if I say that I want a safer policy and go on with sarsa its fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15e2e846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(environment, episodes, alpha, gamma, expl_func, expl_param):\n",
    "    \"\"\"\n",
    "    Performs the SARSA algorithm for a specific environment\n",
    "    \n",
    "    Args:\n",
    "        environment: OpenAI gym environment\n",
    "        episodes: number of episodes for training\n",
    "        alpha: alpha parameter\n",
    "        gamma: gamma parameter\n",
    "        expl_func: exploration function (epsilon_greedy, softmax)\n",
    "        expl_param: exploration parameter (epsilon, T)\n",
    "    \n",
    "    Returns:\n",
    "        (policy, rewards, lengths): final policy, rewards for each episode [array], length of each episode [array]\n",
    "    \"\"\"\n",
    "\n",
    "    q = np.zeros((environment.observation_space.n, environment.action_space.n))\n",
    "    rewards = np.zeros(episodes)\n",
    "    lengths = np.zeros(episodes)\n",
    "    \n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        s = environment.reset()\n",
    "        a = expl_func(q,s,expl_param)\n",
    "\n",
    "        step = 1\n",
    "        while step is not None:\n",
    "            step = environment.step(a)\n",
    "            if step is None:\n",
    "                break\n",
    "            next_state, reward, _, _ = step\n",
    "            a_1 = expl_func(q,next_state,expl_param)\n",
    "\n",
    "            q[s][a] = q[s][a] + alpha*(reward + gamma*(q[next_state][a_1] - q[s][a]))\n",
    "            s = next_state\n",
    "            a = a_1\n",
    "            \n",
    "            if s == environment.goalstate:\n",
    "                break\n",
    "\n",
    "        rewards[episode] += reward\n",
    "        lengths[episode] += episode\n",
    "    \n",
    "    policy = [0 for _ in range(environment.observation_space.n)]\n",
    "    for s in range(environment.observation_space.n):\n",
    "        for a in environment.actions:\n",
    "            policy[s] += np.argmax(q[s][a])\n",
    "\n",
    "\n",
    "    policy = q.argmax(axis=1)\n",
    "    return policy, rewards, lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ce36292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['S' 'L' 'L' 'P']\n",
      " ['L' 'P' 'L' 'P']\n",
      " ['L' 'L' 'L' 'G']]\n",
      "\n",
      "Policy:\n",
      " [['U' 'L' 'L' 'L']\n",
      " ['U' 'L' 'D' 'L']\n",
      " ['L' 'L' 'L' 'L']] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "episodes = 1000\n",
    "gamma = 1\n",
    "alpha = .3\n",
    "epsilon = .1\n",
    "\n",
    "\n",
    "env = tools.gym.make('LavaFloor-v0')\n",
    "env.render()\n",
    "print() \n",
    "\n",
    "policy, rewards, lengths = sarsa(env, episodes, alpha, gamma, epsilon_greedy, epsilon)\n",
    "print(\"Policy:\\n {} \\n\".format(np.vectorize(env.actions.get)(policy.reshape(env.shape))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9846752",
   "metadata": {},
   "source": [
    "# We notice that is mostly trying to avoid the pit than going towards the goal! (THATS WHY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72ae25a",
   "metadata": {},
   "source": [
    "### 1.2) Find a way to force the agent to choose the shortest path towards the goal. You can change these parameters: episodes, alpha, gamma exploration function and exploration parameter. \n",
    "\n",
    "##### Hint: you should tune a parameter to consider the immediate reward rather than the long-term one... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0733b3",
   "metadata": {},
   "source": [
    "DECREASE GAMMA! REACHES THE TREASURE AS SOON AS POSSIBLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86059fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['S' 'L' 'L' 'P']\n",
      " ['L' 'P' 'L' 'P']\n",
      " ['L' 'L' 'L' 'G']]\n",
      "\n",
      "Policy:\n",
      " [['L' 'L' 'L' 'L']\n",
      " ['U' 'L' 'R' 'L']\n",
      " ['U' 'L' 'L' 'L']] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "episodes = 200\n",
    "gamma =  .1\n",
    "alpha =  .3\n",
    "epsilon = .1\n",
    "\n",
    "env.render()\n",
    "print() \n",
    "\n",
    "# Q-Learning or SARSA epsilon greedy\n",
    "env = tools.gym.make('LavaFloor-v0')\n",
    "p, r, l = sarsa(env, episodes, alpha, gamma, epsilon_greedy, epsilon)\n",
    "print(\"Policy:\\n {} \\n\".format(np.vectorize(env.actions.get)(p.reshape(env.shape))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbff4e4",
   "metadata": {},
   "source": [
    "### 1.3) Consider and environment with states {A, B, C, D}, actions {r, l} where states {A, D} are terminal. Consider the following sequence of learning episodes:\n",
    "* E1: (B, r, C, −1),(C, r, C, −1),(C, r, D, +1)\n",
    "* E2: (B, r, C, −1),(C, r, D, +1)\n",
    "* E3: (B, l, A, +5)\n",
    "* E4: (B, l, B, −1),(B, l, B, −1),(B, l, A, +5)\n",
    "### Compute v(s) for all non-terminal states by using a sample-based evaluation approach (i.e., computing the values with the function reported below). Assume $\\alpha$ = .5 and $\\gamma$ = 1.\n",
    "### $V(s) = (1- \\alpha) \\cdot V(s) + \\alpha \\cdot (r + \\gamma \\cdot V(s'))$ \n",
    "### where $s$ is the state under consideration (first element of each tuple), $s'$  (third element of each tuple ) the state reached by starting from $s$ and performing the action $a$ (second element of each tuple). And finally, $r$ is the reward (last element of each tuple)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "017a0627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 5, 'B': 6.90625, 'C': 1.375, 'D': 1}\n"
     ]
    }
   ],
   "source": [
    "episodes = {1: [('B', 'r', 'C', -1), ('C', 'r', 'C', -1),('C', 'r', 'D', 1)], \n",
    "            2: [('B', 'r', 'C', -1), ('C', 'r', 'D', 1)], \n",
    "            3: [('B', 'l', 'A', 5)], \n",
    "            4: [('B', 'l', 'B', -1),('B', 'l', 'B', -1),('B', 'l', 'A', 5)]}\n",
    "\n",
    "v = {'A': 5, 'B': 0, 'C': 0, 'D': 1}\n",
    "alpha = 0.5\n",
    "gamma = 1\n",
    "\n",
    "\n",
    "for episode, values in episodes.items():\n",
    "    for s,a,s_1,r in values:\n",
    "        v[s] = (1 - alpha)*v[s] + alpha*(r + gamma*(v[s_1]))\n",
    "\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfc8f7c",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc5f593",
   "metadata": {},
   "source": [
    "Consider the figure below where **S=$(1,2)$** and **G=$(3,1)$** are the starting and goal positions respectively. Consider the problem of finding a minimum cost path from S to G assuming the agent can move in the four directions (if there is no\n",
    "obstacle) and that each movement has a unitary cost. The environment is deterministic. Answer the following questions:\n",
    "\n",
    "<img src=\"images/ex2.png\" alt=\"ex2\" style=\"zoom: 40%;\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed0ba51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['C' 'C' 'C' 'C' 'C']\n",
      " ['C' 'C' 'S' 'W' 'C']\n",
      " ['C' 'W' 'W' 'W' 'C']\n",
      " ['C' 'G' 'C' 'C' 'C']]\n"
     ]
    }
   ],
   "source": [
    "env = tools.gym.make('SmallMaze-v0')\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7062b99",
   "metadata": {},
   "source": [
    "### 2.1) Verify by using the code developed in the lab that the Manhattan distance (l1_norm) is a  *consistent* heuristic in this environment. In particular, you should implement a function that checks whether for every couple of state $(s,s')$ (where $s'$ is a successor state of $s$), the consistency condition is verified. The function should return true if the heuristic is consistent and false otherwise. Recall that every action has a cost of 1...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de60399",
   "metadata": {},
   "source": [
    "The best way to solve the excercise is to just say why I know that heuristic is consistent and then to code the consistency proof: I have to check that every state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0352c701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consistency_proof(goalpos,s,s_1):\n",
    "    if(Heu.l1_norm(s,goalpos) + 1) >= Heu.l1_norm(s_1,goalpos): #consistency: h(i) <= c(i,i+1) + h(i + 1)\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f6ff235e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_heuristic(env):\n",
    "    goalpos = env.state_to_pos(env.goalstate)\n",
    "    for s in range(env.observation_space.n):\n",
    "\n",
    "        if env.grid[s] == \"W\":\n",
    "            continue\n",
    "        for action in range(env.action_space.n):\n",
    "            s_pos = env.state_to_pos(s)\n",
    "            s_1_pos = env.state_to_pos(env.sample(s,action))\n",
    "            if(not consistency_proof(goalpos,s_pos,s_1_pos)):\n",
    "                print(f\"The heuristic is not consistent for {s} and {s_1}\")\n",
    "                return False\n",
    "    print(\"The heuristic is consistent!\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e932af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The heuristic is consistent!\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "goalpos = env.state_to_pos(env.goalstate)\n",
    "print(check_heuristic(env))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97399378",
   "metadata": {},
   "source": [
    "### 2.2) Consider the A* algorithm and assume want to achieve optimality. Based on the *consistent* heuristics of Section 2.1, state whether it is best to use a graph search or tree search strategy. Motivate your answer and show the results of A* execution and the differences between the two versions in terms of the computed solution, number of nodes generated and maximum number of nodes in memory. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217679b5",
   "metadata": {},
   "source": [
    "Since we have a consistent heuristic we have to choose a graph search version to achieve optimality. We know only if tree search + admissible heuristic or graph search + consistent heuristic => optimality of A*. In all other situations, we cannot guarantee optimality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c74ec4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def present_with_higher_cost(queue, node):\n",
    "    if node.state in queue:\n",
    "        if queue[node.state].value > node.value: \n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9e5fcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def astar_tree_search(environment):\n",
    "    \"\"\"\n",
    "    A* Tree search\n",
    "    \n",
    "    Args:\n",
    "        problem: OpenAI Gym environment\n",
    "        \n",
    "    Returns:\n",
    "        (path, time_cost, space_cost): solution as a path and stats.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    goalpos = environment.state_to_pos(environment.goalstate)\n",
    "    queue = PriorityQueue()\n",
    "    startpos = env.state_to_pos(env.startstate)\n",
    "    queue.add(Node(environment.startstate,None,1,Heu.l1_norm(startpos,goalpos)+1))\n",
    "    \n",
    "    time_cost = 1\n",
    "    space_cost = 1\n",
    "    \n",
    "    while True:\n",
    "        if queue.is_empty(): \n",
    "            return None\n",
    "        \n",
    "        # Retrieve node from the queue\n",
    "        node = queue.remove()  \n",
    "        if node.state == environment.goalstate: \n",
    "            return build_path(node), time_cost, space_cost\n",
    "        \n",
    "        # Look around\n",
    "        for action in range(environment.action_space.n):\n",
    "            #added path cost that's an heuristic\n",
    "            child_state = environment.sample(node.state, action)\n",
    "            child_pos = env.state_to_pos(child_state)\n",
    "            child = Node(child_state, node, node.pathcost + 1, Heu.l1_norm(child_pos, goalpos) + node.pathcost +1)\n",
    "            time_cost += 1\n",
    "            \n",
    "            queue.add(child)\n",
    "                \n",
    "        space_cost = max(space_cost, len(queue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c23b470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def astar_graph_search(environment):\n",
    "    \"\"\"\n",
    "    A* Graph Search\n",
    "    \n",
    "    Args:\n",
    "        problem: OpenAI Gym environment\n",
    "        \n",
    "    Returns:\n",
    "        (path, time_cost, space_cost): solution as a path and stats.\n",
    "    \"\"\"\n",
    "    \n",
    "    goalpos = environment.state_to_pos(environment.goalstate)\n",
    "    queue = PriorityQueue()\n",
    "    startpos = env.state_to_pos(env.startstate)\n",
    "    queue.add(Node(environment.startstate,None,1,Heu.l1_norm(startpos,goalpos)+1))\n",
    "    \n",
    "    \n",
    "    explored = set()\n",
    "    time_cost = 1\n",
    "    space_cost = 1\n",
    "    \n",
    "    while True:\n",
    "        if queue.is_empty(): \n",
    "            return None\n",
    "        \n",
    "        # Retrieve node from the queue\n",
    "        node = queue.remove()  \n",
    "        if node.state == environment.goalstate: \n",
    "            return build_path(node), time_cost, space_cost\n",
    "        \n",
    "        explored.add(node.state)\n",
    "        \n",
    "        # Look around\n",
    "        for action in range(environment.action_space.n):\n",
    "            #added path cost that's an heuristic\n",
    "            child_state = environment.sample(node.state, action)\n",
    "            child_pos = env.state_to_pos(child_state)\n",
    "            child = Node(child_state, node, node.pathcost + 1, Heu.l1_norm(child_pos, goalpos) + node.pathcost + 1) \n",
    "            time_cost += 1\n",
    "            \n",
    "            if child.state not in queue and child.state not in explored:\n",
    "                queue.add(child)\n",
    "                \n",
    "            elif present_with_higher_cost(queue, child):\n",
    "                queue.replace(child)\n",
    "                \n",
    "        space_cost = max(space_cost, len(queue) + len(explored))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "425883ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution (tree-search): [(1, 1), (1, 0), (2, 0), (3, 0), (3, 1)]\n",
      "N° of nodes explored: 125\n",
      "Max n° of nodes in memory: 94\n",
      "\n",
      "=================================================================\n",
      "\n",
      "Solution (graph-search): [(1, 1), (1, 0), (2, 0), (3, 0), (3, 1)]\n",
      "N° of nodes explored: 29\n",
      "Max n° of nodes in memory: 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = tools.gym.make('SmallMaze-v0')\n",
    "solution_ts, time_ts, memory_ts = astar_tree_search(env)\n",
    "solution_gs, time_gs, memory_gs = astar_graph_search(env)\n",
    "\n",
    "print(\"Solution (tree-search): {}\".format(solution_2_string(solution_ts, env)))\n",
    "print(\"N° of nodes explored: {}\".format(time_ts))\n",
    "print(\"Max n° of nodes in memory: {}\\n\".format(memory_ts))\n",
    "print('='*65)\n",
    "print(\"\\nSolution (graph-search): {}\".format(solution_2_string(solution_gs, env)))\n",
    "print(\"N° of nodes explored: {}\".format(time_gs))\n",
    "print(\"Max n° of nodes in memory: {}\\n\".format(memory_gs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f177b11c",
   "metadata": {},
   "source": [
    "### 2.3) Let us consider BFS, show the path of the optimal solution (avoiding the repetition of states) to achieve the goal. In this scenario can we guarantee that the returned solution is the least cost one? If we used Iterative Deepening Search(IDS) with the same methodology used for BFS can we guarantee a least cost solution? Justify your answer and show the results of the different strategies by using the code developed during the lab. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee43dcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sre_constants import FAILURE\n",
    "from importlib_metadata import NullFinder\n",
    "\n",
    "\n",
    "def BFS_GraphSearch(problem):\n",
    "    \"\"\"\n",
    "    Graph Search BFS\n",
    "    \n",
    "    Args:\n",
    "        problem: OpenAI Gym environment\n",
    "        \n",
    "    Returns:\n",
    "        (path, time_cost, space_cost): solution as a path and stats.\n",
    "    \"\"\"\n",
    "    \n",
    "    node = Node(problem.startstate, None)\n",
    "    time_cost = 1\n",
    "    space_cost = 1\n",
    "    \n",
    "    #initial test of node\n",
    "    if node.state == problem.goalstate:\n",
    "        return build_path(node), time_cost, space_cost\n",
    "\n",
    "    #initialize frontier and explored\n",
    "    frontier = NodeQueue()\n",
    "    frontier.add(node)\n",
    "    explored = []\n",
    "\n",
    "    while not frontier.is_empty():\n",
    "        node = frontier.remove()\n",
    "        explored.append(node.state)\n",
    "        for action in range(problem.action_space.n):\n",
    "            child = Node(problem.sample(node.state,action), node)\n",
    "            time_cost += 1\n",
    "            if child.state not in explored and child.state not in frontier:\n",
    "                if child.state == problem.goalstate:\n",
    "                   return build_path(child), time_cost, space_cost    \n",
    "                frontier.add(child)\n",
    "                if(len(frontier) > space_cost or len(explored) > space_cost):\n",
    "                    space_cost += max(len(frontier), len(explored))\n",
    "    return FAILURE, time_cost, space_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76c11d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution (BFS graph-search): [(1, 1), (1, 0), (2, 0), (3, 0), (3, 1)]\n",
      "N° of nodes explored: 39\n",
      "Max n° of nodes in memory: 15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = tools.gym.make('SmallMaze-v0')\n",
    "solution_gs, time_gs, memory_gs = BFS_GraphSearch(env)\n",
    "print(\"Solution (BFS graph-search): {}\".format(solution_2_string(solution_gs, env)))\n",
    "print(\"N° of nodes explored: {}\".format(time_gs))\n",
    "print(\"Max n° of nodes in memory: {}\\n\".format(memory_gs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6663ad8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DLS(problem, limit, RDLS_Function):\n",
    "    \"\"\"\n",
    "    DLS\n",
    "    \n",
    "    Args:\n",
    "        problem: OpenAI Gym environment\n",
    "        limit: depth limit for the exploration, negative number means 'no limit'\n",
    "        \n",
    "    Returns:\n",
    "        (path, time_cost, space_cost): solution as a path and stats.\n",
    "    \"\"\"\n",
    "        \n",
    "    node = Node(problem.startstate, None)\n",
    "    return RDLS_Function(node, problem, limit, set())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d97dd742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Recursive_DLS_GraphSearch(node, problem, limit, explored):\n",
    "    \"\"\"\n",
    "    Recursive DLS\n",
    "    \n",
    "    Args:\n",
    "        node: node to explore\n",
    "        problem: OpenAI Gym environment\n",
    "        limit: depth limit for the exploration, negative number means 'no limit'limit\n",
    "        explored: completely explored nodes\n",
    "        \n",
    "    Returns:\n",
    "        (path, time_cost, space_cost): solution as a path and stats.\n",
    "    \"\"\"\n",
    "    \n",
    "    #initialize frontier and explored\n",
    "    space_cost = node.depthcost\n",
    "    time_cost = 1 \n",
    "    explored.add(node.state)\n",
    "\n",
    "    if node.state == problem.goalstate:\n",
    "        return build_path(node), time_cost, space_cost\n",
    "    elif limit == 0:\n",
    "        return \"cut_off\", time_cost, space_cost\n",
    "    else:\n",
    "        cut_off_occured = False\n",
    "        for action in range(problem.action_space.n):\n",
    "            child = Node(problem.sample(node.state,action), node)\n",
    "            \n",
    "            if child.state not in explored:\n",
    "                result, time_cost_child, space_cost_child = Recursive_DLS_GraphSearch(child, problem, limit - 1, explored)\n",
    "\n",
    "                space_cost = max(space_cost, space_cost_child)\n",
    "                time_cost += time_cost_child\n",
    "                \n",
    "                if result == \"cut_off\":\n",
    "                    cut_off_occured = True\n",
    "                elif result != FAILURE:\n",
    "                    return result, time_cost, space_cost\n",
    "                \n",
    "        if cut_off_occured:\n",
    "            return \"cut_off\", time_cost, space_cost \n",
    "        else:\n",
    "            return FAILURE, time_cost, space_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1bfe7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example IDS+GRAPH SEARCH\n",
    "def IDS(problem, DLS_Function):\n",
    "    \"\"\"\n",
    "    Iteartive_DLS DLS\n",
    "    \n",
    "    Args:\n",
    "        problem: OpenAI Gym environment\n",
    "        \n",
    "    Returns:\n",
    "        (path, time_cost, space_cost): solution as a path and stats.\n",
    "    \"\"\"\n",
    "        \n",
    "    total_time_cost = 0\n",
    "    total_space_cost = 1\n",
    "    \n",
    "    for i in zero_to_infinity():\n",
    "        result, time_cost, space_cost = DLS(problem,i,DLS_Function)\n",
    "        total_time_cost += time_cost\n",
    "        total_space_cost = max(space_cost, total_space_cost)\n",
    "        if result!=\"cut_off\":\n",
    "            return result, total_time_cost, total_space_cost, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41388738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution (IDS graph-search): [(1, 1), (1, 0), (2, 0), (3, 0), (3, 1)]\n",
      "N° of nodes explored: 37\n",
      "Max n° of nodes in memory: 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = tools.gym.make('SmallMaze-v0')\n",
    "solution_gs, time_gs, memory_gs, iterations_gs = IDS(env, Recursive_DLS_GraphSearch)\n",
    "print(\"Solution (IDS graph-search): {}\".format(solution_2_string(solution_gs, env)))\n",
    "print(\"N° of nodes explored: {}\".format(time_gs))\n",
    "print(\"Max n° of nodes in memory: {}\\n\".format(memory_gs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89f593b",
   "metadata": {},
   "source": [
    "### Discuss the results achieved with respect to the 2.3 question:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe0e580",
   "metadata": {},
   "source": [
    "No, we cannot guarantee that BFS returns the least cost solution (used graph search to avoid repetition of states). If we unluckily explore the wrong node first, with something already being in the frontier we won't find the optimal solution.\\\n",
    "Graph search version of IDS doesn't guarantee optimality either: if we use tree search it will guarantee optimality.\n",
    "\n",
    "### WRONG FIRST ANSWER\n",
    "In this case we can guarantee with BFS because the coast are all uniform (its like UCS) so it guarantees to find the optimal solution. This is not generally guranteed though!\\\n",
    "In this case it doesnt matter if its graph or tree search\n",
    "\n",
    "### CORRECT SECOND ANSWER BUT\n",
    "This gives us the optimal solution in this case: BUT ITS NOT GUARANTEE!\\\n",
    "Graph search version of IDS doesn't guarantee optimality either: if we use tree search it will guarantee optimality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de110226",
   "metadata": {},
   "source": [
    "### A*\n",
    "\n",
    "Its optimal efficient: guarantees optimality with the less node in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bc2c64",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de20f428",
   "metadata": {},
   "source": [
    "Consider the environment displayed in Figure below, where states $(0, 3)$ and $(1, 3)$ are terminal states with reward $+1$ and $−1$ respectively. The agent can move in the four directions. The transition model states that for every state and action the agent has $0.8$ chances of moving in the chosen direction and $0.1$ chances to move in the othogonal directions.The reward model states that for every state, action and successor state the agent pays $−0.04$. Assume that the dicount factor is set to $0.9$. Answer the following questions: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b78047",
   "metadata": {},
   "source": [
    "<img src=\"images/ex3.1.png\" alt=\"ex3\" style=\"zoom: 40%;\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf24b52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['S' 'L' 'L' 'G']\n",
      " ['L' 'W' 'L' 'P']\n",
      " ['L' 'L' 'L' 'L']]\n"
     ]
    }
   ],
   "source": [
    "env = tools.gym.make('VeryBadLavaFloor-v0')\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac451e5",
   "metadata": {},
   "source": [
    "### 3.1) Use one of the methods developed in the lab to compute the value function for the left diagram. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee91851a",
   "metadata": {},
   "source": [
    "Value iteration is the best because we are not actually asking for a policy but for the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "490b94e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(environment, maxiters=300, discount=0.9, max_error=1e-3):\n",
    "    \"\"\"\n",
    "    Performs the value iteration algorithm for a specific environment\n",
    "    \n",
    "    Args:\n",
    "        environment: OpenAI Gym environment\n",
    "        maxiters: timeout for the iterations\n",
    "        discount: gamma value, the discount factor for the Bellman equation\n",
    "        max_error: the maximum error allowd in the utility of any state\n",
    "        \n",
    "    Returns:\n",
    "        policy: 1-d dimensional array of action identifiers where index `i` corresponds to state id `i`\n",
    "    \"\"\"\n",
    "    delta = 0 # maximum change in the utility o any state in an iteration\n",
    "    U_1 = [0 for _ in range(environment.observation_space.n)] # vector of utilities for states S\n",
    "    iters = 0\n",
    "    U = U_1.copy()\n",
    "    \n",
    "    while True:\n",
    "\n",
    "        delta = 0\n",
    "        U = U_1.copy()\n",
    "        \n",
    "        for s in range(environment.observation_space.n):\n",
    "\n",
    "            action_value = [0 for _ in range(environment.action_space.n)]\n",
    "            \n",
    "            for a in range(environment.action_space.n):\n",
    "                for s_1 in range(environment.observation_space.n):\n",
    "                    action_value[a] += environment.T[s,a,s_1]*U[s_1]\n",
    "\n",
    "            \n",
    "\n",
    "            if environment.grid[s] == 'G' or environment.grid[s] == 'P':\n",
    "                 U_1[s] = environment.RS[s]\n",
    "            else:\n",
    "                U_1[s] = environment.RS[s] + discount*max(action_value)\n",
    "\n",
    "            if abs(U_1[s] - U[s]) > delta: \n",
    "                delta = U_1[s] - U[s]\n",
    "\n",
    "        iters += 1\n",
    "\n",
    "        if(delta < max_error * (1 - discount)/discount or iters >= maxiters):\n",
    "            break\n",
    "\n",
    "    return values_to_policy(np.asarray(U), environment),np.asarray(U) # automatically convert the value matrix U to a policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da933217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.50939438  0.64958568  0.79536209  1.          0.39844322  0.\n",
      "  0.48644002 -1.          0.29628832  0.253867    0.34475423  0.12987275]\n",
      "[['R' 'R' 'R' 'L']\n",
      " ['U' 'L' 'U' 'L']\n",
      " ['U' 'R' 'U' 'L']]\n"
     ]
    }
   ],
   "source": [
    "env = tools.gym.make('VeryBadLavaFloor-v0')\n",
    "policy, values = value_iteration(env)\n",
    "policy_render = np.vectorize(env.actions.get)(policy.reshape(env.rows, env.cols))\n",
    "print(values)\n",
    "print(policy_render)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8216357",
   "metadata": {},
   "source": [
    "### Results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66795ad",
   "metadata": {},
   "source": [
    "<img src=\"images/ex3.png\" alt=\"ex3\" style=\"zoom: 40%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b180254a",
   "metadata": {},
   "source": [
    "### 3.2) Consider the right diagram and focus on states $(2, 1)$. State whether the action reported in the right diagram (the blue one) represents the optimal action for that state. Motivate your answer with the code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ebad340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0]\n",
      "The correct action to perform should be: L\n"
     ]
    }
   ],
   "source": [
    "actions = {0: \"L\", 1: \"R\", 2: \"U\", 3: \"D\"}\n",
    "\n",
    "value_function = [0.50939438, 0.64958568, 0.79536209, 1, 0.39844322, 0, 0.48644002, -1, 0.29628832,  0.253867, 0.34475423, 0.12987275]\n",
    "id_start_state = 9\n",
    "gamma = 0.9\n",
    "\n",
    "\n",
    "values_ex = [0, 0, 0, 0]\n",
    "'''\n",
    "YOUR CODE HERE\n",
    "'''\n",
    "    \n",
    "print(np.asarray(values_ex))\n",
    "print(f'The correct action to perform should be: {actions[np.argmax(values_ex)]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611fa5fc",
   "metadata": {},
   "source": [
    "### 3.3) Compute the probability of ending in state (1, 3) if we execute the sequence of actons < Up, Up > from state (2, 2). Motivate your answer reporting the code and the solution printed. The following image shows a diagram to guide the solution process as a hint for you:\n",
    "\n",
    "\n",
    "<img src=\"images/example.png\" alt=\"ex3\" style=\"zoom: 30%;\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2369c26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2) --> (1, 2) --> (1, 3)\n",
      "probs: 0.8 --> 0.1\n",
      "================\n",
      "(2, 2) --> (2, 3) --> (1, 3)\n",
      "probs: 0.1 --> 0.8\n",
      "================\n",
      "\n",
      "Probability:  0.16000000000000003\n"
     ]
    }
   ],
   "source": [
    "id_start_state = 10\n",
    "state = id_start_state\n",
    "actions = {0: \"L\", 1: \"R\", 2: \"U\", 3: \"D\"}\n",
    "\n",
    "prob = 0 #probability of the action that end in pit\n",
    "action = 2\n",
    "probs_fin = 0 #sum of probability\n",
    "\n",
    "for next_state in range(env.observation_space.n):\n",
    "    \n",
    "    if env.T[state, action, next_state] == 0:\n",
    "        continue\n",
    "\n",
    "\n",
    "    probs = env.T[state, action, next_state]\n",
    "\n",
    "    for second_next_state in range(env.observation_space.n):\n",
    "        \n",
    "        if env.T[next_state, action, second_next_state] == 0:\n",
    "            continue\n",
    "\n",
    "        if env.grid[second_next_state] == \"P\": #check that the state is (1,3) i.e., the black pit\n",
    "            \n",
    "            probs *= env.T[next_state, action, second_next_state]\n",
    "\n",
    "            print(f'{env.state_to_pos(state)} --> {env.state_to_pos(next_state)} --> {env.state_to_pos(second_next_state)}')\n",
    "            print(f'probs: {env.T[state, action, next_state]} --> {env.T[next_state, action, second_next_state]}')\n",
    "\n",
    "            probs_fin += probs\n",
    "\n",
    "            print('================')\n",
    "            break\n",
    "    \n",
    "print()\n",
    "print('Probability: ', probs_fin)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455c0594",
   "metadata": {},
   "source": [
    "### 3.4) Consider the following environment where states $(0, 3)$ and $(1, 3)$ are terminal states with reward $+1$ and $−1$ respectively. Assume the transition model is the same one defined above and that the discounted factor is $0.9$ as above. However, now the agent pays $−0.4$ instead of $−0.04$ for every action, state and successor state.  Compute the new value function and the optimal policy for this new environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89237f75",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"images/env_3.4.png\" alt=\"ex3\" style=\"zoom: 40%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "164ca1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.084   -0.35824  0.27392  1.      -1.084    0.      -0.37984 -1.\n",
      " -1.084   -1.084   -1.084   -1.084  ]\n",
      "[['R' 'R' 'R' 'L']\n",
      " ['L' 'L' 'U' 'L']\n",
      " ['L' 'L' 'U' 'U']]\n"
     ]
    }
   ],
   "source": [
    "env = tools.gym.make('NiceLavaFloor-v0')\n",
    "policy, values = value_iteration(env)\n",
    "policy_render = np.vectorize(env.actions.get)(policy.reshape(env.rows, env.cols))\n",
    "print(values)\n",
    "print(policy_render)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e4b4e0",
   "metadata": {},
   "source": [
    "### 3.5) Modify the value iteration parameters so that the policy allows the agent to reach the goal only starting from states $(0,1)$, $(0,2)$ and $(1,2)$, as reported in the image below . Motivate your answer.\n",
    "\n",
    "\n",
    "#### hint: given the negative reward the agent should care about the immediate reward and not the long-term reward.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfd75f5",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"images/result_ex3.5.png\" alt=\"ex3\" style=\"zoom: 40%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6d7eed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.44  -0.44  -0.328  1.    -0.44   0.    -0.44  -1.    -0.44  -0.44\n",
      " -0.44  -0.44 ]\n",
      "[['L' 'R' 'R' 'L']\n",
      " ['L' 'L' 'U' 'L']\n",
      " ['L' 'L' 'L' 'D']]\n"
     ]
    }
   ],
   "source": [
    "env = tools.gym.make('NiceLavaFloor-v0')\n",
    "policy, values =  value_iteration(env, discount=0.1)\n",
    "policy_render = np.vectorize(env.actions.get)(policy.reshape(env.rows, env.cols))\n",
    "print(values)\n",
    "print(policy_render)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 ('planning-lab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "2a6ccb17efef36f5c310ec71f45d46e51258e3e0415c30921d872f5e998c8175"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
